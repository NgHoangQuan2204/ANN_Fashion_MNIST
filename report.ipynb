{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Group information"
      ],
      "metadata": {
        "id": "GY0hf0SNaQ9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of members and Assignments\n",
        "\n",
        "| Student ID | Name | Tasks |\n",
        "| ----------|-------|--------------|\n",
        "| 20120030 | Nguyễn Thiên An | First parallel version. Slides. Reports |\n",
        "| 20120090 | Nguyễn Thế Hoàng | Sequential version. Project managment. Reports. Optimization. Testing. |\n",
        "| 20120146 | Nguyễn Thị Châu Ngọc | Second parallel version. Debugging. |"
      ],
      "metadata": {
        "id": "lVp07-YoaTnY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dl7uj8hyWdZ"
      },
      "source": [
        "# Set up system\n",
        "\n",
        "And running Mini-CNN for runnability testing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOoGG1KsQW2q",
        "outputId": "183ceeda-baec-44da-d164-6d1038b38243"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EXqbXsNlpOF"
      },
      "source": [
        "Check CUDA compiler version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTZtGIK6lsFo",
        "outputId": "5e294dfa-0cf6-47aa-fdb2-5ca6ca81cb63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the location to the `CNN_Parallelization` directory as example below"
      ],
      "metadata": {
        "id": "T9NMNO24aAer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#projectDirRootName = \"path/to/CNN_Parallelization/\" # Change to correct localtion of CNN_Parallelization directory\n",
        "projectDirRootName = \"/content/drive/MyDrive/University/Parallel Computing/Personal/CNN_Parallelization/\""
      ],
      "metadata": {
        "id": "6s2sPf8pZgql"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile executable file\n",
        "\n",
        "Set up for compiling demo.exe. If the demo.exe has been put in the correct location: skip belows cell until you have met cell \"RUN THIS\". But you should re-complie and re-linking objects code again to receive the executable for sure."
      ],
      "metadata": {
        "id": "pHZ3gkmah9Xr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXSFDweYcJ-Q",
        "outputId": "8652abe2-fe31-4a5b-aed7-a0f75f953b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/University/Parallel Computing/Personal/CNN_Parallelization/Source/mini-dnn-cpp-master\n",
            "\u001b[0m\u001b[01;34mbuild\u001b[0m/             \u001b[01;34mdata\u001b[0m/                  LICENSE       \u001b[01;34mtest\u001b[0m/\n",
            "CMakeLists.txt     demo.cc                readme.md     testImplement.cc\n",
            "config.h           demo_Fashion_MNIST.cc  report.ipynb  testImplement.h\n",
            "ConvExperiment.cc  \u001b[01;34mfigure\u001b[0m/                \u001b[01;34msrc\u001b[0m/          \u001b[01;34mthird_party\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%cd {projectDirRootName}Source/mini-dnn-cpp-master\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsjWN5AtcYcK",
        "outputId": "b2f49dcd-7da3-4691-8ebf-84e2e88fa056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/University/Parallel Computing/Personal/CNN_Parallelization/Source/mini-dnn-cpp-master/build\n"
          ]
        }
      ],
      "source": [
        "%rm -r build\n",
        "%mkdir build\n",
        "%cd build\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mc2YkdAqW1_",
        "outputId": "02dd38e0-cea3-4e82-acf1-ed596f01e865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Configuring done (8.1s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/drive/MyDrive/University/Parallel Computing/Personal/CNN_Parallelization/Source/mini-dnn-cpp-master/build\n"
          ]
        }
      ],
      "source": [
        "!cmake .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdwIP5bieHpm",
        "outputId": "4988e3c6-1e7e-4e0e-de98-ad3e6675cf5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -5%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/mnist.cc.o\u001b[0m\n",
            "[  0%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/network.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/layer/ave_pooling.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/layer/conv.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object src/CMakeFiles/MiniDNNLib.dir/layer/cuda_utilities.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/layer/fully_connected.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/layer/max_pooling.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/layer/relu.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/layer/sigmoid.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/layer/softmax.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/loss/cross_entropy_loss.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/loss/mse_loss.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/MiniDNNLib.dir/optimizer/sgd.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX static library libMiniDNNLib.a\u001b[0m\n",
            "[ 63%] Built target MiniDNNLib\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/demo.dir/ConvExperiment.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/demo.dir/demo.cc.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/demo.dir/demo_Fashion_MNIST.cc.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/demo.dir/testImplement.cc.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable demo\u001b[0m\n",
            "[ 89%] Built target demo\n"
          ]
        }
      ],
      "source": [
        "!make"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RUN THIS"
      ],
      "metadata": {
        "id": "NpnwvdMtBKL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the `./demo.exe` for activating LeNet-5 running on the test set. The `demo.exe` can run with multiple version of Convolutional layer (currently there are 3 versions which are indexed from 0 to 3) on the same test set, by passing follwing arguments:\n",
        "\n",
        "- First argument: the index of the first version you want to execute.\n",
        "- Second argument: the index of final version that `demo.exe` will implement.\n",
        "- Third argument: if `0`, the `demo.exe` only executes the first version. If `1`, the `demo.exe` executes all versions from first one to the final one.\n",
        "\n",
        "Following is the list of implemented versions and theirs index (The first and second arguments can gain below number):\n",
        "\n",
        "- `0` - The sequential version: The forward of Convolutional layer is sequential in the input rolling stage.\n",
        "- `1` - The first parallel version: The forward of Convolutional layer is parallelized in the input rolling stage.\n",
        "- `2` - The second parallel version: The forward of Convolutional layer is parallelizedd in the matrix multiplication between input features and layer's weights.\n",
        "- Any other number - The original implementation of Convolutional layer which is provided as is by authors of `mini-dnn-cpp` project.\n"
      ],
      "metadata": {
        "id": "0_gyYvtHBRI4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xUKBF8M3fVCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86de81e5-e505-42ba-ae93-d051a69991f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/University/Parallel Computing/Personal/CNN_Parallelization/Source/mini-dnn-cpp-master/build\n",
            "CMakeCache.txt  \u001b[0m\u001b[01;34mCMakeFiles\u001b[0m/  cmake_install.cmake  demo  Makefile  \u001b[01;34msrc\u001b[0m/\n",
            "../data/fashion-mnist/\n",
            "mnist train number: 60000\n",
            "mnist test number: 10000\n",
            "Parameters loaded\n",
            "\n",
            "Current version: -1\n",
            "\n",
            "Test case 0 passed\n",
            "Test case 1 passed\n",
            "Test case 2 passed\n",
            "Test case 3 passed\n",
            "Test case 4 passed\n",
            "Test case 5 passed\n",
            "Test case 6 passed\n",
            "Test case 7 passed\n",
            "Test case 8 passed\n",
            "Test case 9 passed\n",
            "Test cases passed\n",
            "Test case 0 passed\n",
            "Test case 1 passed\n",
            "Test case 2 passed\n",
            "Test case 3 passed\n",
            "Test case 4 passed\n",
            "Test case 5 passed\n",
            "Test case 6 passed\n",
            "Test case 7 passed\n",
            "Test case 8 passed\n",
            "Test case 9 passed\n",
            "Test cases passed\n",
            "\n",
            "\n",
            "Test time: 78152.4\n",
            "Test acc: 0.8297\n",
            "\n",
            "------------------------------------------\n",
            "\n",
            "Parameters loaded\n",
            "\n",
            "Current version: 0\n",
            "\n",
            "Test case 0 passed\n",
            "Test case 1 passed\n",
            "Test case 2 passed\n",
            "Test case 3 passed\n",
            "Test case 4 passed\n",
            "Test case 5 passed\n",
            "Test case 6 passed\n",
            "Test case 7 passed\n",
            "Test case 8 passed\n",
            "Test case 9 passed\n",
            "Test cases passed\n",
            "Test case 0 passed\n",
            "Test case 1 passed\n",
            "Test case 2 passed\n",
            "Test case 3 passed\n",
            "Test case 4 passed\n",
            "Test case 5 passed\n",
            "Test case 6 passed\n",
            "Test case 7 passed\n",
            "Test case 8 passed\n",
            "Test case 9 passed\n",
            "Test cases passed\n",
            "\n",
            "\n",
            "Test time: 73255.4\n",
            "Test acc: 0.8297\n",
            "\n",
            "------------------------------------------\n",
            "\n",
            "Parameters loaded\n",
            "\n",
            "Current version: 1\n",
            "\n",
            "Test case 0 passed\n",
            "Test case 1 passed\n",
            "Test case 2 passed\n",
            "Test case 3 passed\n",
            "Test case 4 passed\n",
            "Test case 5 passed\n",
            "Test case 6 passed\n",
            "Test case 7 passed\n",
            "Test case 8 passed\n",
            "Test case 9 passed\n",
            "Test cases passed\n",
            "Test case 0 passed\n",
            "Test case 1 passed\n",
            "Test case 2 passed\n",
            "Test case 3 passed\n",
            "Test case 4 passed\n",
            "Test case 5 passed\n",
            "Test case 6 passed\n",
            "Test case 7 passed\n",
            "Test case 8 passed\n",
            "Test case 9 passed\n",
            "Test cases passed\n",
            "\n",
            "\n",
            "Test time: 41349.5\n",
            "Test acc: 0.8297\n",
            "\n",
            "------------------------------------------\n",
            "\n",
            "Parameters loaded\n",
            "\n",
            "Current version: 2\n",
            "\n",
            "Test case 0 passed\n",
            "Test case 1 passed\n",
            "Test case 2 passed\n",
            "Test case 3 passed\n",
            "Test case 4 passed\n",
            "Test case 5 passed\n",
            "Test case 6 passed\n",
            "Test case 7 passed\n",
            "Test case 8 passed\n",
            "Test case 9 passed\n",
            "Test cases passed\n",
            "Test case 0 passed\n",
            "Test case 1 passed\n",
            "Test case 2 passed\n",
            "Test case 3 passed\n",
            "Test case 4 passed\n",
            "Test case 5 passed\n",
            "Test case 6 passed\n",
            "Test case 7 passed\n",
            "Test case 8 passed\n",
            "Test case 9 passed\n",
            "Test cases passed\n",
            "\n",
            "\n",
            "Test time: 31260.9\n",
            "Test acc: 0.8297\n",
            "\n",
            "------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd {projectDirRootName}/Source/mini-dnn-cpp-master/build\n",
        "%ls\n",
        "!chmod 755 demo\n",
        "!./demo -1 2 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdGh_rTRXK63"
      },
      "source": [
        "# Problem definition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Layer - LeNet-5 Model\n",
        "\n",
        "As described in the previous seminar and slides, the Convolutional layer (Conv. layer for short) is the most basic and important architecture used in the Computer Vision fields. It can reduce the size of input images, as well as capture the lower to higher information of the image and transfer those knowlege to top layers of the model for downstream tasks.\n",
        "\n",
        "The Conv. layer uses the kernel matrix and convolution operation to achieve the task. However, the time complexity of that computation is really high, especially with large number of images and high quality images. As soon as GPU was utilized for parallelize the computing, people has found a way to speed up the Conv. layer implementation. This project try to use a small subset of CUDA API to test the parallelization capability of GPU in Conv. layer computing.\n",
        "\n",
        "We built LeNet-5 models with the configuration as suggested in project description. The below figure visualizes the details of LeNet-5."
      ],
      "metadata": {
        "id": "5hmNCFI1oazC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1OQGrV0-1fK5N5naB-JfvXeb0X_WzeDG1)"
      ],
      "metadata": {
        "id": "wzDEOZRspDto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic idea"
      ],
      "metadata": {
        "id": "sUv9gutpqSVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the given starter project [2] as the staring point for parallelizing the forward phase of Conv. layer. Here, the author uses the matrix multiplication between the weight matrix of Conv. layer and unrolled version of batch input images for computing output of Conv. layer, as shown in below image ([1])."
      ],
      "metadata": {
        "id": "T0fyYuuhqU9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1XMlgD_H-7o6lfOp7WG_5DIXvrt7U-QWk)"
      ],
      "metadata": {
        "id": "F8BdMkR-qz24"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnQZPzrYXTS7"
      },
      "source": [
        "## Input/Output and Terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4w8O92wXrSq"
      },
      "source": [
        "### Input layout\n",
        "\n",
        "A mini-batch of multiple input images (input features) $X$ as the tensor has shape $(N, C, H, W)$ where:\n",
        "\n",
        "* $N$: number of samples in a mini-batch\n",
        "* $C$: number of input feature maps.\n",
        "* $H$: height of each input feature map (or simply just the height of each input image in pixels).\n",
        "* $W$: width of each input feature map (or simply just the weight of each input image in pixels)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2f_Bv2m54P_"
      },
      "source": [
        "### Output layout\n",
        "\n",
        "The output features after applying CNN to $X$. It is an array $Y$ as the tensor has shape $(N, M, H_\\text{out}, W_\\text{out})$ where:\n",
        "\n",
        "* $N$: number of samples in a mini-batch\n",
        "* $M$: number of output feature maps of a CNN layer.\n",
        "* $H_\\text{out}$: height of each output feature map (it is often that $H_\\text{out} = H - K + 1$).\n",
        "* $W_\\text{out}$: width of each output feature map (it is often that $W_\\text{out} = W - K + 1$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWyhdqsj5_DD"
      },
      "source": [
        "### Filter-bank layout\n",
        "\n",
        "The matrix $W$ contains all filer maxtrix that is used for one CNN layer (or simply just the weigth matrix of a CNN layer).  It is a tensor has shape $(M, C, K, K)$ where:\n",
        "\n",
        "* $K$: the size of a filter matrix (or kernel matrix).\n",
        "\n",
        "With an input image has $C$ input feature maps (or $C$ color chanels) and the CNN layer produces $M$ output feature maps from that input feature maps, we need $C \\times M$ kernel matrix with size $K$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm02TxsL6CBN"
      },
      "source": [
        "### The Unrolled-X\n",
        "\n",
        "We unroll the matrix X. As a result, we can get all elements that are required for computing all output feature maps from a single input image, with just a single matrix multiplication step between $X$ and $W$. For now, just know that $X_\\text{unroll}$ is retrieved from $X$, and it has shape $(C, K, K, H_\\text{out}, W_\\text{out})$, where:\n",
        "\n",
        "* $(C, K, K)$: the \"height\" of $X_\\text{unroll}$. It is the number of elements in $X$ that we need to compute an output feature map element (that is $C \\times K \\times K$),\n",
        "* $(H_\\text{out}, W_\\text{out})$: the \"width\" of $X_\\text{unroll}$. It is the number of elements in an output feature map, that is $H_\\text{out} \\times W_\\text{out}$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contribution (Results)\n",
        "\n"
      ],
      "metadata": {
        "id": "gRdIFwvWM7ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"tableResult\"></a>Table of Results of Implemented Version of LeNet-5 (Conv. Layer)"
      ],
      "metadata": {
        "id": "kJVs-ZQvNM4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| ID  | Program specification | Time execution - Forward only<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) (ms) | Test Accuracy | Time of running Test set<a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2) (ms) |\n",
        "| --- | --------------------- | ---------------------------------- | ----------------------- | - |\n",
        "| A0 | LeNet-5 - Original - MNIST || 0.9737 ||\n",
        "| **A1** | **LeNet-5 - Original - Fashion-MNSIT** | **$ \\approx 1040.63$** | **0.8297** | **78152.4** |\n",
        "| A2 | LeNet-5 - Original - Transpose Matrix Multiplication | $ \\approx 985.677$ | 0.8194 ||\n",
        "| **B0** | **LeNet-5 - Sequential (Im2Col) - ~~Transpose Matrix Multiplication~~** | **$ \\approx 1115.75 \\rightarrow 916.248$** | **0.8297** | **73255.4** |\n",
        "| C0 | LeNet-5 - Parallel Version 1 (Im2Col) - Get unrolled image matrix avoid naive copy | $ \\approx 696.307 $ | 0.8297 ||\n",
        "| **C1** | **LeNet-5 - Parallel Version 1 (Im2Col) - Same as (C0), but get unrolled image results directly from dynamic memory to Matrix object** | **$ \\approx 539.801 $** | **0.8297** | **41349.5** |\n",
        "| **D0** | **LeNet-5 - Parallel Version 2 (MatMul) - Save matrix while multiplying as column-major order** | | **0.8297** | **31260.9** |\n",
        "\n",
        "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) : Avarage of all forward passes in first epoch.\n",
        "\n",
        "<a name=\"cite_note-2\"></a>2. [^](#cite_ref-2): The Test set contains 10000 samples. The results is the best one after re-run the executable files 10 times."
      ],
      "metadata": {
        "id": "Y8tsDOdBNHKf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE29-G9xfltj"
      },
      "source": [
        "# Version 0: Naive implementation (Sequential Version)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment, verification, timing strategy"
      ],
      "metadata": {
        "id": "9ayoMDnmU9up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1VrSjbh-X32nRMI0uwcH_hC-ejDFrEPgO)"
      ],
      "metadata": {
        "id": "Yy-EHVSwIg-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For all following experiments, we need to have these objects:\n",
        "\n",
        "1. **Weight matrix of LeNet-5**\n",
        "  \n",
        "  To save time for experimenting, we do not need to train the model again and again. We only need to pass the test set to the new version and get the test accuracy. To avoid repeating training and have proper ground truth weigth matrix, we train original version of the LeNet-5 on the full training set with standard configuration, and then save the model weights to file (As drawn in Pre-Train Phase in above figure).\n",
        "\n",
        "  For any new version of Convolutional layer, we only need to load that saved model weights from file to the model, then run on test set and record observed results (As drawn in Testing Phase in above figure).\n",
        "\n",
        "2. **Test cases**\n",
        "\n",
        "  In many cases, the accuracy of the test set is not enough for concluding if the new implementation of Convolutional layer is sufficient, or even worse it may lead to misleading results due to random process of the models. To verify the correctness of the new Convolutional layer, we have built 20 test cases for independent testing of forward step of Conv. layer.\n",
        "\n",
        "  The 20 test cases is built randomly. 10 of them has shape $(1, 28, 28)$ (the size of batch is not considered while doing test cases) which are the same shape of one input image of the first Conv. layer. 10 remaining test cases has shape $(6, 24, 24)$ which are the same shape of one input features map of the second Conv. layer. All of elements of each test case matrix are in range $[0, 255]$. The expected output of test cases is produced by passing them through the forward function of the Conv. layer of the original LeNet-5 version.\n",
        "\n",
        "  For each new version Conv. layer, we pass all 20 test cases through the forward step and then, compare the produced results with the expected output test cases (as drawn in purple ovals in the below figure). The pass/fail status of each test case is reported while the program is running.\n",
        "\n",
        "  All of test cases and theirs expected output is stored in `CNN_Parallelization/Source/mini-dnn-cpp-master/test` directory.\n",
        "\n",
        "The time measurement of each version is the time to forward all samples in the test cases. In other words, we measure how long the model can process and return the result of the given Fashion-MNIST test set."
      ],
      "metadata": {
        "id": "D7nrBQ18Bpo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1XQFtDvNYVNP2CFcGI5lSTQAnpi1iMRIb)"
      ],
      "metadata": {
        "id": "BB8vlmqyHdcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation and Specification"
      ],
      "metadata": {
        "id": "W-ABLNW0SBzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This version is implemented in the `conv.cc` file. We create `forwardVerion_0()` to activate this forward version, and `unroll()` as the simplified version of original unroll input features maps `im2col()`.\n",
        "\n",
        "The below is the pseudo-code of this implementation:\n",
        "\n",
        "```[C++]\n",
        "foreach input-feature c of image\n",
        "  foreach position (p, q) of each area\n",
        "    foreach possible-position-of-applied-kernel-area-KxK area of c\n",
        "      insert element (p, q) to X-unrolled\n",
        "```\n",
        "\n",
        "Simply speaking, we process each input feature $c$ ($c \\in [0, C)$) of an image. For each $c$, we want to collect elemen at position ($p, q$) ($p, q \\in [0, K)$) of the current area in input feature which is being applied kernel matrix. The ($p, q$) elements is inserted in the row $p \\times K + q$ of the X-unrolled matrix. We do the same for each next area when the kernel matrix is slided, which the next elements will be inserted in following columns of X-unrolled matrix.\n",
        "\n",
        "When we have done for an input feature $c$, we just go to the next row of X-unrolled matrix and repeat the second and third for loop as before."
      ],
      "metadata": {
        "id": "qzhduS3Zcge5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "TYctpl0GVdVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test accuracy and Testing time can be refered to [this table, version A1](#tableResult).\n",
        "\n",
        "Additionally, this version passed all the designated test cases."
      ],
      "metadata": {
        "id": "I4R9RN5ZPT9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 1: Basic Parallelize"
      ],
      "metadata": {
        "id": "H4_2QCbDQUQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic idea"
      ],
      "metadata": {
        "id": "HxGO4fdTQfrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the version 0, we can see that each time an $x$-th element belongs to an applied kernel matrix area $i$ has been inserted into the X-unrolled, that area $i$ has to wait until all other $x$-th elements of others area $j$ ($j \\geq i$) have also been inserted. We can seperate the insertion of each individual area by letting each thread processes an applied kernel matrix area from input features."
      ],
      "metadata": {
        "id": "65k7csmEQhkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation and Specification"
      ],
      "metadata": {
        "id": "T6shgRTghuBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This version is implemented in the `conv.cc` and `cuda_utilities.cu` file. We create `forwardVersion_1()` to activate this forward version, and `unrollKernel_1()` as the parallelized kernel version of original unroll input features maps `im2col()`. `forwardVersion_1()` activate `unrollKernel_1()` by calling `unrollGPUWrapper()` in `cuda_utilities.cu`.\n",
        "\n",
        "This version parallelize the third for loop of above sequential version. Now, each thread only need to executes the second for loop, which means each thread inserts the input feature elements belong to the same area applied kernel matrix into the X-unrolled. Because there are $C$ input feature of each image, and there are $H_\\text{out} \\times W_\\text{out}$ possible area which can be applied kernel matrix, we need a grid contains $C \\times H_\\text{out} \\times W_\\text{out}$ threads."
      ],
      "metadata": {
        "id": "MciRT-3hhw0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "6l0gSEGfTphC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test accuracy and Testing time can be refered to [this table, version C1](#tableResult).\n",
        "\n",
        "Additionally, this version passed all the designated test cases."
      ],
      "metadata": {
        "id": "P7ouovfgTs9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 2: Matrix Multiplication Parallelization"
      ],
      "metadata": {
        "id": "5TClW2rNSK2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic idea"
      ],
      "metadata": {
        "id": "KjbivtwXSSKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By seperating the insertion of each applied kernel matrix area in input features, we can speed up the execution a lot. However, there is still a major bottleneck in the forward step of Conv. layer after unrolling the input matrix: the matrix multiplication between the X-unrolled and weight matrix of the Conv. layer.\n",
        "\n",
        "After unrolling phase, it is common that we often need to multiply two big size matrix with each other. That is an opportunity to speed up the program by parallelizing the matrix multiplication."
      ],
      "metadata": {
        "id": "rJb93kfyST3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation and Specification"
      ],
      "metadata": {
        "id": "HaCrUceuTj0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This version is implemented in the `conv.cc` and `cuda_utilities.cu` file. We create `forwardVersion_2()` to activate this forward version, and `matrixMultiplicationKernel_1()` as the parallelized kernel version of matrix multiplication. `forwardVersion_2()` activate `matrixMultiplicationKernel_1()` by calling `matrixMultiplicationGPUWrapper()` in `cuda_utilities.cu`.\n",
        "\n",
        "The basic idea of parallelized matrix multiplicatin we have used is shared tiled memory matrix multiplication (As implmeneted in Lab 2).\n",
        "\n",
        "To generalize for any multiplication of any size matrix, we check two following conditions [1, Chapter 4]:\n",
        "\n",
        "- If the thread belong to the valid element of the result matrix $Y$, we add result to that position in $Y$. Otherwise, we do not assign.\n",
        "- If the thread is transfering the valid element from $W$ or $X-unrolled$ to shared memory, we let it do that. If not, we assign that position in the shared memory as 0."
      ],
      "metadata": {
        "id": "mROZktWShhW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "4dcZ5SMWT9Wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test accuracy and Testing time can be refered to [this table, version D0](#tableResult).\n",
        "\n",
        "Additionally, this version passed all the designated test cases."
      ],
      "metadata": {
        "id": "RJArI6GVT_cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflection"
      ],
      "metadata": {
        "id": "k55VcZYmV4Kn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final results\n",
        "\n",
        "- Implement the simplified sequential version of Conv. Layer based on starter project, which is the stardard for further parallelization steps.\n",
        "- Implement two version of parallelized forward phase of Conv. Layer.\n",
        "- Post-Seminar modification: implement the independent test cases processes for more reliable correctness verification of forward phase of Conv. layer."
      ],
      "metadata": {
        "id": "TF9V6BgqV65s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsolved"
      ],
      "metadata": {
        "id": "aQP1AxauWtbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Utilise the effectiveness of global and share memory pattern access of CUDA API.\n",
        "- Utilise the multiple executing streams of CUDA API.\n",
        "- Parallelize other layers of LeNet-5."
      ],
      "metadata": {
        "id": "h40P6vDHWxBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Difficulties\n",
        "\n",
        "- The CUDA driver cannot compile the Eigen library: We detach the source code which need the Matrix class and the CUDA source code into seperate files. The CUDA host and kernel execution is as normal functions and put in `cuda_utilities.cu` file, so that other ordinary source files can call wrapped CUDA function, and CUDA does not need to know about Eigen library.\n",
        "- Because of above problem, the data moving between Matrix class in normal source files and dynamic memory in CUDA source files can be cubersome and may cause bottleneck if we copy raw data from memory to a Matrix class object directly: We implement the CUDA part such that the Matrix class objects can get the result back directly through Map class, without need to do raw copying or transposing. This way-around solution is not straight forward and may cause unaligned or un-coalesced memory accessing, but it is much better than bottle neck of manually copying data between Matrix and dynamic memory.\n",
        "- Furious bug: CUDA API may make debug process more difficult. We often meet some trivial bugs (such as: uninitialized memory, reset memory, etc) but they are easily hidden below the execution of CUDA threads.\n",
        "- It is quite not trivial to describe by word the most detail implementation we have done with the source code.  "
      ],
      "metadata": {
        "id": "anC6mevVXDVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "jtkoRjztmhDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] D. Kirk and Wen-Mei Hwu, Programming massively parallel processors : a hands-on approach. Burlington, Massachusetts: Morgan Kaufmann Elsevier, 2017.\n",
        "\n",
        "[2] K. Han, “iamhankai/mini-dnn-cpp,” GitHub, Jan. 08, 2024. https://github.com/iamhankai/mini-dnn-cpp (accessed Jan. 13, 2024).\n",
        "‌"
      ],
      "metadata": {
        "id": "td3NNqMInMp9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}